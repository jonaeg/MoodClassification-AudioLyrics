{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis notebook\n",
    "\n",
    "\n",
    "For this analysis, we only take the argmax combined model into account, since it has the best accuracy from the models developed in the notebook 'Bimodel_modular.ipynb', and compare its performance with the single model performances, as well as their performances between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models, data, and perform the predictions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast\n",
    "from joblib import dump, load\n",
    "\n",
    "df_train = pd.read_csv('database/LaA_train.csv')\n",
    "df_test = pd.read_csv('database/LaA_test.csv')\n",
    "df_train.head()\n",
    "\n",
    "embeddings_audio_train = df_train['audio_embedding'].apply(ast.literal_eval).apply(np.array)\n",
    "x_audio_train = np.stack(embeddings_audio_train.values)\n",
    "embeddings_audio_test = df_test['audio_embedding'].apply(ast.literal_eval).apply(np.array)\n",
    "x_audio_test = np.stack(embeddings_audio_test.values)\n",
    "\n",
    "embeddings_lyrics_train = df_train['lyrics_embedding'].apply(ast.literal_eval).apply(np.array)\n",
    "x_lyrics_train = np.stack(embeddings_lyrics_train.values)\n",
    "embeddings_lyrics_test = df_test['lyrics_embedding'].apply(ast.literal_eval).apply(np.array)\n",
    "x_lyrics_test = np.stack(embeddings_lyrics_test.values)\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_svms = True\n",
    "if use_pretrained_svms: \n",
    "    name_svm_audio = 'models/SVM_audio.joblib'\n",
    "    name_svm_lyrics = 'models/SVM_lyrics.joblib'\n",
    "    svm_classifier_audio = load(name_svm_audio)\n",
    "    svm_classifier_lyrics = load(name_svm_lyrics)\n",
    "else:\n",
    "    svm_classifier_lyrics = SVC(kernel='rbf', C=1, gamma='auto', probability=True)\n",
    "    svm_classifier_lyrics.fit(x_lyrics_train, y_train)\n",
    "    print(\"Lyrics SVM trained\")\n",
    "\n",
    "    svm_classifier_audio = SVC(kernel='rbf', C=1, gamma='auto', probability=True)\n",
    "    svm_classifier_audio.fit(x_audio_train, y_train)\n",
    "    print(\"Audio SVM trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "# arg max model\n",
    "y_pred_test_prob_lyrics = svm_classifier_lyrics.predict_proba(x_lyrics_test)\n",
    "y_pred_test_prob_audio = svm_classifier_audio.predict_proba(x_audio_test)\n",
    "y_pred_train_prob_lyrics = svm_classifier_lyrics.predict_proba(x_lyrics_train)\n",
    "y_pred_train_prob_audio = svm_classifier_audio.predict_proba(x_audio_train)\n",
    "\n",
    "y_pred_lyrics = np.argmax(y_pred_test_prob_lyrics, axis=1)\n",
    "y_pred_audio = np.argmax(y_pred_test_prob_audio, axis=1)\n",
    "\n",
    "y_pred_max_combined = np.argmax(y_pred_test_prob_lyrics + y_pred_test_prob_audio, axis=1)\n",
    "acc_max_combined = accuracy_score(y_test, y_pred_max_combined)\n",
    "print(f\"Accuracy: {acc_max_combined:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check on how many labels the two models differ (per emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Angry:\n",
      "  Both Agree & Correct: (Lower bound for combination) 0.05\n",
      "  Audio Correct, Lyrics Wrong: 0.14\n",
      "  Lyrics Correct, Audio Wrong: 0.09\n",
      "  Upper Bound Correct: 0.29\n",
      "  Combined Correct: 0.14\n",
      "\n",
      "Emotion Happy:\n",
      "  Both Agree & Correct: (Lower bound for combination) 0.45\n",
      "  Audio Correct, Lyrics Wrong: 0.20\n",
      "  Lyrics Correct, Audio Wrong: 0.21\n",
      "  Upper Bound Correct: 0.86\n",
      "  Combined Correct: 0.72\n",
      "\n",
      "Emotion Relaxed:\n",
      "  Both Agree & Correct: (Lower bound for combination) 0.01\n",
      "  Audio Correct, Lyrics Wrong: 0.10\n",
      "  Lyrics Correct, Audio Wrong: 0.04\n",
      "  Upper Bound Correct: 0.14\n",
      "  Combined Correct: 0.04\n",
      "\n",
      "Emotion Sad:\n",
      "  Both Agree & Correct: (Lower bound for combination) 0.35\n",
      "  Audio Correct, Lyrics Wrong: 0.18\n",
      "  Lyrics Correct, Audio Wrong: 0.26\n",
      "  Upper Bound Correct: 0.79\n",
      "  Combined Correct: 0.63\n",
      "\n",
      "Estimated Upper Bound Accuracy: 0.59\n",
      "Our accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "emotion_map = {0: 'Angry', 1: 'Happy', 2: 'Relaxed', 3: 'Sad'}\n",
    "\n",
    "total_correct = 0\n",
    "total_possible = 0\n",
    "\n",
    "for emotion in [0,1,2,3]:\n",
    "    emotion_mask = y_test == emotion\n",
    "    audio_correct = y_pred_audio == y_test\n",
    "    lyrics_correct = y_pred_lyrics == y_test\n",
    "    comb_correct = y_pred_max_combined == y_test\n",
    "    \n",
    "    both_correct = np.logical_and(audio_correct, lyrics_correct) & emotion_mask\n",
    "    audio_only_correct = np.logical_and(audio_correct, np.logical_not(lyrics_correct)) & emotion_mask\n",
    "    lyrics_only_correct = np.logical_and(lyrics_correct, np.logical_not(audio_correct)) & emotion_mask\n",
    "    \n",
    "    results[emotion] = {\n",
    "        'both_agree_and_correct': np.mean(both_correct[emotion_mask]),\n",
    "        'audio_correct_lyrics_wrong': np.mean(audio_only_correct[emotion_mask]),\n",
    "        'lyrics_correct_audio_wrong': np.mean(lyrics_only_correct[emotion_mask]),\n",
    "        'comb_correct': np.mean(comb_correct[emotion_mask]),\n",
    "    }\n",
    "    correct_predictions = np.logical_or(both_correct, np.logical_or(audio_only_correct, lyrics_only_correct))\n",
    "\n",
    "    # Calculate the total number of correct predictions and the total number of possible correct predictions\n",
    "    total_correct += np.sum(correct_predictions)\n",
    "    total_possible += np.sum(emotion_mask)\n",
    "\n",
    "\n",
    "for emotion, result in results.items():\n",
    "    print(f\"Emotion {emotion_map[emotion]}:\")\n",
    "    print(f\"  Both Agree & Correct: (Lower bound for combination) {result['both_agree_and_correct']:.2f}\")\n",
    "    print(f\"  Audio Correct, Lyrics Wrong: {result['audio_correct_lyrics_wrong']:.2f}\")\n",
    "    print(f\"  Lyrics Correct, Audio Wrong: {result['lyrics_correct_audio_wrong']:.2f}\")\n",
    "    print(f\"  Upper Bound Correct: {np.sum([result['both_agree_and_correct'], result['audio_correct_lyrics_wrong'], result['lyrics_correct_audio_wrong']]):.2f}\")\n",
    "    print(f\"  Combined Correct: {result['comb_correct']:.2f}\\n\")\n",
    "\n",
    "# Calculate and print the overall estimated upper bound accuracy\n",
    "upper_bound_accuracy = total_correct / total_possible\n",
    "print(f\"Estimated Upper Bound Accuracy: {upper_bound_accuracy:.2f}\")\n",
    "print(f\"Our accuracy: {acc_max_combined:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: We see the model works, because the combined model chooses in most of the cases the one that's right (e.g. combined correct in the worst case would be both agree and correct, in the best case (both agree and correct + one of both agrees), and we're closer to the second case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check how certain the models are about their decisions for each emotion, depending on if they are right or wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Model Confidence Analysis:\n",
      "Average max confidence scores for being correct, audio model: {0: 0.41396013280337673, 1: 0.4492064958765077, 2: 0.39492113481037383, 3: 0.42812359094689195}\n",
      "Average max confidence scores for being wrong, audio model: {0: 0.4136322585454098, 1: 0.3969532960695512, 2: 0.41689713716473575, 3: 0.4110479570157514}\n",
      "\n",
      "Lyrics Model Confidence Analysis:\n",
      "Average max confidence scores for being correct, lyrics model: {0: 0.40514261119215694, 1: 0.46574352118434575, 2: 0.37666522055591656, 3: 0.41128707896323763}\n",
      "Average max confidence scores for being wrong, lyrics model: {0: 0.4127315567337244, 1: 0.39216714606247377, 2: 0.4188959055308722, 3: 0.4023000962658397}\n"
     ]
    }
   ],
   "source": [
    "def analyze_model_confidence(y_test, y_pred_probs, y_pred, emotion_labels):\n",
    "    correct_confidences = {emotion: [] for emotion in emotion_labels}\n",
    "    incorrect_confidences = {emotion: [] for emotion in emotion_labels}\n",
    "    \n",
    "    # Iterate through predictions and true labels\n",
    "    for i, (true_label, pred_label) in enumerate(zip(y_test, y_pred)):\n",
    "        # Confidence for the predicted label\n",
    "        confidence = y_pred_probs[i][pred_label]\n",
    "        \n",
    "        # Correct predictions\n",
    "        if true_label == pred_label:\n",
    "            correct_confidences[true_label].append(confidence)\n",
    "        # Incorrect predictions\n",
    "        else:\n",
    "            incorrect_confidences[true_label].append(confidence)\n",
    "    \n",
    "    # Calculate average confidences\n",
    "    average_confidences = {\n",
    "        'correct': {emotion: np.mean(correct_confidences[emotion]) if correct_confidences[emotion] else 0 for emotion in emotion_labels},\n",
    "        'incorrect': {emotion: np.mean(incorrect_confidences[emotion]) if incorrect_confidences[emotion] else 0 for emotion in emotion_labels}\n",
    "    }\n",
    "    \n",
    "    return average_confidences\n",
    "\n",
    "# Example usage\n",
    "emotion_labels = np.unique(y_test)  # Assuming y_test is available\n",
    "y_pred_audio = np.argmax(y_pred_test_prob_audio, axis=1)\n",
    "y_pred_lyrics = np.argmax(y_pred_test_prob_lyrics, axis=1)\n",
    "\n",
    "# Analyze confidence for audio model\n",
    "audio_confidence_analysis = analyze_model_confidence(y_test, y_pred_test_prob_audio, y_pred_audio, emotion_labels)\n",
    "# Analyze confidence for lyrics model\n",
    "lyrics_confidence_analysis = analyze_model_confidence(y_test, y_pred_test_prob_lyrics, y_pred_lyrics, emotion_labels)\n",
    "\n",
    "print(\"Audio Model Confidence Analysis:\")\n",
    "print(f\"Average max confidence scores for being correct, audio model: {audio_confidence_analysis['correct']}\")\n",
    "print(f\"Average max confidence scores for being wrong, audio model: {audio_confidence_analysis['incorrect']}\")\n",
    "print(\"\\nLyrics Model Confidence Analysis:\")\n",
    "print(f\"Average max confidence scores for being correct, lyrics model: {lyrics_confidence_analysis['correct']}\")\n",
    "print(f\"Average max confidence scores for being wrong, lyrics model: {lyrics_confidence_analysis['incorrect']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to mention in the presentation\n",
    "\n",
    "- We checked that the dataset is balanced\n",
    "- We checked that the models are not horribly over fitting (hopefully)\n",
    "- Explanation of audio features and how we acquired the data\n",
    "- Explanation of lyrical features and how we acquired it \n",
    "- Explanation of the arousal/valence scale\n",
    "- Short explanation SVM, mention scaling\n",
    "- Analysis of the two independent models\n",
    "- Introduction to ensemble learning, present different techniques\n",
    "- Discussion, what went wrong, what could be done better (both for the simple models and the bimodal model)\n",
    "- Perhaps comparison with Deezer paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions we should be able to answer\n",
    "\n",
    "- How could you improve the accuracy of your model?\n",
    "\n",
    "- Why is the model doing better on some emotions than on others?\n",
    "\n",
    "- Is there any issue with fixing the test train split beforehand?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
